# Stubs for delira.data_loading.data_manager (Python 3)
#
# NOTE: This dynamically typed stub was automatically generated by stubgen.

import numpy as np
from batchgenerators.dataloading import SlimDataLoaderBase
from delira.data_loading.data_loader import BaseDataLoader
from delira.data_loading.dataset import AbstractDataset
from delira.data_loading.sampler import AbstractSampler
from multiprocessing import Queue
from typing import Any, Callable, Dict, Iterable, Iterator, List, Optional, Type, Union

logger: Any

class Augmenter:
    _batchsize: Any = ...
    _augmenter: Any = ...
    _sampler: Any = ...
    _sampler_queues: Any = ...
    _queue_id: int = ...
    def __init__(self, data_loader: BaseDataLoader, transforms: Optional[Callable], n_process_augmentation: int, sampler: Type[AbstractSampler], sampler_queues: List[int], num_cached_per_queue: int=..., seeds: Union[List[int], Iterable[int], int]=..., **kwargs: Any) -> Any: ...
    def __iter__(self) -> Iterator: ...
    def _next_queue(self) -> Queue: ...
    def __next__(self) -> Dict[str, np.ndarray]: ...
    def next(self) -> Dict[str, np.ndarray]: ...
    @staticmethod
    def __identity_fn(*args: Any, **kwargs: Any) -> None: ...
    def _fn_checker(self, function_name: str) -> Any: ...
    @property
    def _start(self) -> Callable: ...
    def restart(self) -> Callable: ...
    def _finish(self) -> Any: ...
    @property
    def num_batches(self) -> int: ...
    @property
    def num_processes(self) -> int: ...
    def __del__(self) -> None: ...

class BaseDataManager:
    _batch_size: Any = ...
    _n_process_augmentation: Any = ...
    _transforms: Any = ...
    _data_loader_cls: Any = ...
    _dataset: Any = ...
    _sampler: Any = ...
    batch_size: Any = ...
    n_process_augmentation: Any = ...
    transforms: Any = ...
    data_loader_cls: Any = ...
    dataset: Any = ...
    sampler: Any = ...
    def __init__(self, data: Union[str, AbstractDataset], batch_size: int, n_process_augmentation: int, transforms: Optional[Callable], sampler_cls: Type[AbstractSampler]=..., sampler_kwargs: Optional[dict]=..., data_loader_cls: Optional[Type[BaseDataLoader]]=..., dataset_cls: Optional[Type[AbstractDataset]]=..., load_fn: Callable=..., from_disc: bool=..., **kwargs: Any) -> Any: ...
    def get_batchgen(self, seed: int=...) -> Augmenter: ...
    def get_subset(self, indices: Union[List, Iterable]) -> Any: ...
    def update_state_from_dict(self, new_state: dict) -> None: ...
    def train_test_split(self, *args: Any, **kwargs: Any) -> Tuple[Any, Any]: ...
    @property
    def batch_size(self) -> int: ...
    @batch_size.setter
    def batch_size(self, new_batch_size: int) -> None: ...
    @property
    def n_process_augmentation(self) -> int: ...
    @n_process_augmentation.setter
    def n_process_augmentation(self, new_process_number: int) -> None: ...
    @property
    def transforms(self) -> Callable: ...
    @transforms.setter
    def transforms(self, new_transforms: Optional[Callable]) -> None: ...
    @property
    def data_loader_cls(self) -> Type[BaseDataLoader]: ...
    @data_loader_cls.setter
    def data_loader_cls(self, new_loader_cls: Type[SlimDataLoaderBase]) -> None: ...
    @property
    def dataset(self) -> AbstractDataset: ...
    @dataset.setter
    def dataset(self, new_dataset: AbstractDataset) -> None: ...
    @property
    def sampler(self) -> AbstractSampler: ...
    @sampler.setter
    def sampler(self, new_sampler: Union[AbstractSampler, Type[AbstractSampler]]) -> None: ...
    @property
    def n_samples(self) -> int: ...
    @property
    def n_batches(self) -> int: ...
