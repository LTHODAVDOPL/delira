{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# Classification with Delira and SciKit-Learn - A very short introduction\n",
    "*Author: Justus Schock* \n",
    "\n",
    "*Date: 31.07.2019*\n",
    "\n",
    "This Example shows how to set up a basic classification model and experiment using SciKit-Learn.\n",
    "\n",
    "Let's first setup the essential hyperparameters. We will use `delira`'s `Parameters`-class for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Michael/miniconda3/envs/deliradev/lib/python3.7/site-packages/chainer/_environment_check.py:41: UserWarning: Accelerate has been detected as a NumPy backend library.\n",
      "vecLib, which is a part of Accelerate, is known not to work correctly with Chainer.\n",
      "We recommend using other BLAS libraries such as OpenBLAS.\n",
      "For details of the issue, please see\n",
      "https://docs.chainer.org/en/stable/tips.html#mnist-example-does-not-converge-in-cpu-mode-on-mac-os-x.\n",
      "\n",
      "Please be aware that Mac OS X is not an officially supported OS.\n",
      "\n",
      "  ''')  # NOQA\n",
      "/Users/Michael/miniconda3/envs/deliradev/lib/python3.7/site-packages/google/protobuf/descriptor.py:47: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from google.protobuf.pyext import _message\n",
      "/Users/Michael/miniconda3/envs/deliradev/lib/python3.7/site-packages/google/protobuf/internal/well_known_types.py:788: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  collections.MutableMapping.register(Struct)\n",
      "/Users/Michael/miniconda3/envs/deliradev/lib/python3.7/site-packages/tensorflow/python/util/nest.py:824: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  _pywrap_tensorflow.RegisterType(\"Sequence\", _collections.Sequence)\n",
      "/Users/Michael/miniconda3/envs/deliradev/lib/python3.7/site-packages/tensorflow/python/training/checkpointable/util.py:448: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  class _ObjectIdentitySet(collections.MutableSet):\n"
     ]
    }
   ],
   "source": [
    "logger = None\n",
    "from delira.training import Parameters\n",
    "import sklearn\n",
    "params = Parameters(fixed_params={\n",
    "    \"model\": {},\n",
    "    \"training\": {\n",
    "        \"batch_size\": 64, # batchsize to use\n",
    "        \"num_epochs\": 10, # number of epochs to train\n",
    "        \"optimizer_cls\": None, # optimization algorithm to use\n",
    "        \"optimizer_params\": {}, # initialization parameters for this algorithm\n",
    "        \"losses\": {}, # the loss function\n",
    "        \"lr_sched_cls\": None,  # the learning rate scheduling algorithm to use\n",
    "        \"lr_sched_params\": {}, # the corresponding initialization parameters\n",
    "        \"metrics\": {\"mae\": sklearn.metrics.mean_absolute_error} # and some evaluation metrics\n",
    "    }\n",
    "}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Since we did not specify any metric, only the `CrossEntropyLoss` will be calculated for each batch. Since we have a classification task, this should be sufficient. We will train our network with a batchsize of 64 by using `Adam` as optimizer of choice.\n",
    "\n",
    "## Logging and Visualization\n",
    "To get a visualization of our results, we should monitor them somehow. For logging we will use `Tensorboard`. Per default the logging directory will be the same as our experiment directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "\n",
    "## Data Preparation\n",
    "### Loading\n",
    "Next we will create some fake data. For this we use the `ClassificationFakeData`-Dataset, which is already implemented in `deliravision`. To avoid getting the exact same data from both datasets, we use a random offset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from deliravision.data.fakedata import ClassificationFakeData\n",
    "dataset_train = ClassificationFakeData(num_samples=1000, \n",
    "                                       img_size=(3, 32, 32), \n",
    "                                       num_classes=10)\n",
    "dataset_val = ClassificationFakeData(num_samples=100, \n",
    "                                     img_size=(3, 32, 32), \n",
    "                                     num_classes=10,\n",
    "                                     rng_offset=10001\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### Augmentation\n",
    "For Data-Augmentation we will apply a few transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from batchgenerators.transforms import RandomCropTransform, \\\n",
    "                                        ContrastAugmentationTransform, Compose\n",
    "from batchgenerators.transforms.spatial_transforms import ResizeTransform\n",
    "from batchgenerators.transforms.sample_normalization_transforms import ZeroMeanUnitVarianceTransform\n",
    "\n",
    "transforms = Compose([\n",
    "    RandomCropTransform(24), # Perform Random Crops of Size 24 x 24 pixels\n",
    "    ResizeTransform(32), # Resample these crops back to 32 x 32 pixels\n",
    "    ContrastAugmentationTransform(), # randomly adjust contrast\n",
    "    ZeroMeanUnitVarianceTransform()\n",
    "]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "With these transformations we can now wrap our datasets into datamanagers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from delira.data_loading import BaseDataManager, SequentialSampler, RandomSampler\n",
    "\n",
    "manager_train = BaseDataManager(dataset_train, params.nested_get(\"batch_size\"),\n",
    "                                transforms=transforms,\n",
    "                                sampler_cls=RandomSampler,\n",
    "                                n_process_augmentation=4)\n",
    "\n",
    "manager_val = BaseDataManager(dataset_val, params.nested_get(\"batch_size\"),\n",
    "                              transforms=transforms,\n",
    "                              sampler_cls=SequentialSampler,\n",
    "                              n_process_augmentation=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Model\n",
    "\n",
    "After we have done that, we can specify our model: We will use a very simple MultiLayer Perceptron here. \n",
    "In opposite to other backends, we don't need to provide a custom implementation of our model, but we can simply use it as-is. It will be automatically wrapped by `SklearnEstimator`, which can be subclassed for more advanced usage.\n",
    "\n",
    "## Training\n",
    "Now that we have defined our network, we can finally specify our experiment and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c2ad8627acc47a1b3387422abd3fb19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Creating unique targets to estimate classes', style=ProgressS…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1:   0%|          | 0/16 [00:00<?, ? batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 16/16 [00:02<00:00,  6.48 batch/s]\n",
      "Validate: 100%|██████████| 100/100 [00:00<00:00, 317.50 sample/s]\n",
      "Epoch 2: 100%|██████████| 16/16 [00:02<00:00,  6.58 batch/s]\n",
      "Validate: 100%|██████████| 100/100 [00:00<00:00, 332.72 sample/s]\n",
      "Epoch 3: 100%|██████████| 16/16 [00:02<00:00,  6.66 batch/s]\n",
      "Validate: 100%|██████████| 100/100 [00:00<00:00, 323.69 sample/s]\n",
      "Epoch 4: 100%|██████████| 16/16 [00:02<00:00,  6.40 batch/s]\n",
      "Validate: 100%|██████████| 100/100 [00:00<00:00, 327.42 sample/s]\n",
      "Epoch 5: 100%|██████████| 16/16 [00:02<00:00,  6.56 batch/s]\n",
      "Validate: 100%|██████████| 100/100 [00:00<00:00, 322.37 sample/s]\n",
      "Epoch 6: 100%|██████████| 16/16 [00:02<00:00,  6.46 batch/s]\n",
      "Validate: 100%|██████████| 100/100 [00:00<00:00, 323.34 sample/s]\n",
      "Epoch 7: 100%|██████████| 16/16 [00:02<00:00,  6.54 batch/s]\n",
      "Validate: 100%|██████████| 100/100 [00:00<00:00, 322.27 sample/s]\n",
      "Epoch 8: 100%|██████████| 16/16 [00:02<00:00,  6.47 batch/s]\n",
      "Validate: 100%|██████████| 100/100 [00:00<00:00, 313.69 sample/s]\n",
      "Epoch 9: 100%|██████████| 16/16 [00:02<00:00,  6.49 batch/s]\n",
      "Validate: 100%|██████████| 100/100 [00:00<00:00, 320.97 sample/s]\n",
      "Epoch 10: 100%|██████████| 16/16 [00:02<00:00,  6.58 batch/s]\n",
      "Validate: 100%|██████████| 100/100 [00:00<00:00, 318.73 sample/s]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning) # ignore UserWarnings raised by dependency code\n",
    "warnings.simplefilter(\"ignore\", FutureWarning) # ignore FutureWarnings raised by dependency code\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from delira.training import SklearnExperiment\n",
    "\n",
    "if logger is not None:\n",
    "    logger.info(\"Init Experiment\")\n",
    "experiment = SklearnExperiment(params, MLPClassifier,\n",
    "                               name=\"ClassificationExample\",\n",
    "                               save_path=\"./tmp/delira_Experiments\",\n",
    "                               key_mapping={\"X\": \"X\"},\n",
    "                               gpu_ids=[0])\n",
    "experiment.save()\n",
    "\n",
    "model = experiment.run(manager_train, manager_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Congratulations, you have now trained your first Classification Model using `delira`, we will now predict a few samples from the testset to show, that the networks predictions are valid (for now, this is done manually, but we also have a `Predictor` class to automate stuff like this):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-56ed5d2a6acc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# get image from current batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm.auto import tqdm # utility for progress bars\n",
    "\n",
    "preds, labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(len(dataset_val))):\n",
    "        img = dataset_val[i][\"data\"] # get image from current batch\n",
    "        img_tensor = img.astype(np.float) # create a tensor from image, push it to device and add batch dimension\n",
    "        pred_tensor = model(img_tensor) # feed it through the network\n",
    "        pred = pred_tensor.argmax(1).item() # get index with maximum class confidence\n",
    "        label = np.asscalar(dataset_val[i][\"label\"]) # get label from batch\n",
    "        if i % 1000 == 0:\n",
    "            print(\"Prediction: %d \\t label: %d\" % (pred, label)) # print result\n",
    "        preds.append(pred)\n",
    "        labels.append(label)\n",
    "        \n",
    "# calculate accuracy\n",
    "accuracy = (np.asarray(preds) == np.asarray(labels)).sum() / len(preds)\n",
    "print(\"Accuracy: %.3f\" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
