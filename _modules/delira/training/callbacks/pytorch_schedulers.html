

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>delira.training.callbacks.pytorch_schedulers &mdash; delira 0.3.0 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/graphviz.css" type="text/css" />
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 

  
  <script src="../../../../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../../../index.html">
          

          
            
            <img src="../../../../_static/delira.svg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.3.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../getting_started.html">Getting started</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../getting_started.html#installation">Installation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../getting_started.html#backends">Backends</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Tutorials:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorial_delira.html">Delira Introduction</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../tutorial_delira.html#loading-data">Loading Data</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../tutorial_delira.html#the-dataset">The Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tutorial_delira.html#the-dataloader">The Dataloader</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tutorial_delira.html#the-datamanager">The Datamanager</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tutorial_delira.html#sampler">Sampler</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../tutorial_delira.html#models">Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../tutorial_delira.html#init"><code class="docutils literal notranslate"><span class="pre">__init__</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tutorial_delira.html#closure"><code class="docutils literal notranslate"><span class="pre">closure</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tutorial_delira.html#prepare-batch"><code class="docutils literal notranslate"><span class="pre">prepare_batch</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../tutorial_delira.html#abstract-networks-for-specific-backends">Abstract Networks for specific Backends</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../tutorial_delira.html#pytorch">PyTorch</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorial_delira.html#forward"><code class="docutils literal notranslate"><span class="pre">forward</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorial_delira.html#id1"><code class="docutils literal notranslate"><span class="pre">prepare_batch</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorial_delira.html#closure-example"><code class="docutils literal notranslate"><span class="pre">closure</span> <span class="pre">example</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tutorial_delira.html#other-examples">Other examples</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../tutorial_delira.html#training">Training</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../tutorial_delira.html#parameters">Parameters</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tutorial_delira.html#trainer">Trainer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tutorial_delira.html#experiment">Experiment</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../tutorial_delira.html#logging">Logging</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../tutorial_delira.html#multistreamhandler"><code class="docutils literal notranslate"><span class="pre">MultiStreamHandler</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tutorial_delira.html#logging-with-visdom-the-trixi-loggers">Logging with <code class="docutils literal notranslate"><span class="pre">Visdom</span></code> - The <code class="docutils literal notranslate"><span class="pre">trixi</span></code> Loggers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../tutorial_delira.html#types-of-visdomhandlers">Types of VisdomHandlers</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../tutorial_delira.html#more-examples">More Examples</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../classification_pytorch.html">Classification with Delira - A very short introduction</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../classification_pytorch.html#logging-and-visualization">Logging and Visualization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../classification_pytorch.html#data-preparation">Data Preparation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../classification_pytorch.html#loading">Loading</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../classification_pytorch.html#augmentation">Augmentation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../classification_pytorch.html#training">Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../classification_pytorch.html#see-also">See Also</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../gan_pytorch.html">Generative Adversarial Nets with Delira - A very short introduction</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../gan_pytorch.html#hyperparameters">HyperParameters</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../gan_pytorch.html#logging-and-visualization">Logging and Visualization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../gan_pytorch.html#data-preparation">Data Preparation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../gan_pytorch.html#loading">Loading</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../gan_pytorch.html#augmentation">Augmentation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../gan_pytorch.html#training">Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../gan_pytorch.html#see-also">See Also</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../segmentation_2d_pytorch.html">Segmentation in 2D using U-Nets with Delira - A very short introduction</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../segmentation_2d_pytorch.html#logging-and-visualization">Logging and Visualization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../segmentation_2d_pytorch.html#data-praparation">Data Praparation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../segmentation_2d_pytorch.html#loading">Loading</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../segmentation_2d_pytorch.html#augmentation">Augmentation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../segmentation_2d_pytorch.html#training">Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../segmentation_2d_pytorch.html#see-also">See Also</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../segmentation_3d_pytorch.html">Segmentation in 3D using U-Nets with Delira - A very short introduction</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../segmentation_3d_pytorch.html#logging-and-visualization">Logging and Visualization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../segmentation_3d_pytorch.html#data-praparation">Data Praparation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../segmentation_3d_pytorch.html#loading">Loading</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../segmentation_3d_pytorch.html#augmentation">Augmentation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../segmentation_3d_pytorch.html#training">Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../segmentation_3d_pytorch.html#see-also">See Also</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">API Documentation:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../_api/_build/modules.html">API Documentation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../_api/_build/delira/delira.html">Delira</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../_api/_build/delira/data_loading/data_loading.html">Data Loading</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../_api/_build/delira/data_loading/arbitrary_data.html">Arbitrary Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../_api/_build/delira/data_loading/nii.html">Nii</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../_api/_build/delira/data_loading/sampler.html">Sampler</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../_api/_build/delira/delira.io.html">IO</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../_api/_build/delira/logging/logging.html">Logging</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../_api/_build/delira/logging/handlers.html"><span class="hidden-section">MultiStreamHandler</span></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../_api/_build/delira/logging/handlers.html#trixihandler"><span class="hidden-section">TrixiHandler</span></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../_api/_build/delira/models/models.html">Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../_api/_build/delira/models/classification.html">Classification</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../_api/_build/delira/models/gan.html">Generative Adversarial Networks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../_api/_build/delira/models/segmentation.html">Segmentation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../_api/_build/delira/training/training.html">Training</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../_api/_build/delira/training/parameters.html">Parameters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../_api/_build/delira/training/trainer.html">Network Trainer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../_api/_build/delira/training/experiment.html">Experiment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../_api/_build/delira/training/callbacks.html">Callbacks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../_api/_build/delira/training/losses.html">Losses</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../_api/_build/delira/training/utils.html"><span class="hidden-section">AurocMetricPyTorch</span></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../_api/_build/delira/training/utils.html#accurarcymetricpytorch"><span class="hidden-section">AccurarcyMetricPyTorch</span></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../_api/_build/delira/training/utils.html#pytorch-batch-to-numpy"><span class="hidden-section">pytorch_batch_to_numpy</span></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../_api/_build/delira/training/utils.html#pytorch-tensor-to-numpy"><span class="hidden-section">pytorch_tensor_to_numpy</span></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../_api/_build/delira/training/utils.html#float-to-pytorch-tensor"><span class="hidden-section">float_to_pytorch_tensor</span></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../_api/_build/delira/training/utils.html#create-optims-default-pytorch"><span class="hidden-section">create_optims_default_pytorch</span></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../_api/_build/delira/training/utils.html#create-optims-gan-default-pytorch"><span class="hidden-section">create_optims_gan_default_pytorch</span></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../_api/_build/delira/training/utils.html#create-optims-default-tf"><span class="hidden-section">create_optims_default_tf</span></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../_api/_build/delira/delira.utils.html">Utilities</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../_api/_build/delira/class_hierarchy.html">Class Hierarchy Diagrams</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/justusschock/delira">GitHub</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">delira</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../../index.html">Module code</a> &raquo;</li>
        
      <li>delira.training.callbacks.pytorch_schedulers</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for delira.training.callbacks.pytorch_schedulers</h1><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">.abstract_callback</span> <span class="k">import</span> <span class="n">AbstractCallback</span>

<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">torch.optim.lr_scheduler</span> <span class="k">import</span> <span class="n">ReduceLROnPlateau</span><span class="p">,</span> <span class="n">CosineAnnealingLR</span><span class="p">,</span> \
        <span class="n">ExponentialLR</span><span class="p">,</span> <span class="n">LambdaLR</span><span class="p">,</span> <span class="n">MultiStepLR</span><span class="p">,</span> <span class="n">StepLR</span>

<div class="viewcode-block" id="DefaultPyTorchSchedulerCallback"><a class="viewcode-back" href="../../../../_api/_build/delira/training/callbacks.html#delira.training.callbacks.DefaultPyTorchSchedulerCallback">[docs]</a>    <span class="k">class</span> <span class="nc">DefaultPyTorchSchedulerCallback</span><span class="p">(</span><span class="n">AbstractCallback</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Implements a Callback, which `at_epoch_end` function is suitable for most</span>
<span class="sd">        schedulers</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
            <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">            Parameters</span>
<span class="sd">            ----------</span>
<span class="sd">            *args :</span>
<span class="sd">                Arbitrary Positional Arguments</span>
<span class="sd">            **kwargs :</span>
<span class="sd">                Arbitrary Keyword Arguments</span>

<span class="sd">            &quot;&quot;&quot;</span>
            <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span> <span class="o">=</span> <span class="kc">None</span>

<div class="viewcode-block" id="DefaultPyTorchSchedulerCallback.at_epoch_end"><a class="viewcode-back" href="../../../../_api/_build/delira/training/callbacks.html#delira.training.callbacks.DefaultPyTorchSchedulerCallback.at_epoch_end">[docs]</a>        <span class="k">def</span> <span class="nf">at_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
            <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">            Executes a single scheduling step</span>

<span class="sd">            Parameters</span>
<span class="sd">            ----------</span>
<span class="sd">            trainer : :class:`PyTorchNetworkTrainer`</span>
<span class="sd">                the trainer class, which can be changed</span>
<span class="sd">            **kwargs :</span>
<span class="sd">                additional keyword arguments</span>

<span class="sd">            Returns</span>
<span class="sd">            -------</span>
<span class="sd">            :class:`PyTorchNetworkTrainer`</span>
<span class="sd">                modified trainer</span>

<span class="sd">            &quot;&quot;&quot;</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">epoch</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;curr_epoch&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
            <span class="k">return</span> <span class="n">trainer</span></div></div>

<div class="viewcode-block" id="ReduceLROnPlateauCallback"><a class="viewcode-back" href="../../../../_api/_build/delira/training/callbacks.html#delira.training.callbacks.ReduceLROnPlateauCallback">[docs]</a>    <span class="k">class</span> <span class="nc">ReduceLROnPlateauCallback</span><span class="p">(</span><span class="n">DefaultPyTorchSchedulerCallback</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Wraps PyTorch&#39;s `ReduceLROnPlateau` Scheduler as Callback</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;min&#39;</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                     <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">threshold_mode</span><span class="o">=</span><span class="s1">&#39;rel&#39;</span><span class="p">,</span>
                     <span class="n">cooldown</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">min_lr</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">):</span>
            <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">            Parameters</span>
<span class="sd">            ----------</span>
<span class="sd">            optimizer : Optimizer</span>
<span class="sd">                Wrapped optimizer.</span>
<span class="sd">            mode : str</span>
<span class="sd">                One of `min`, `max`. In `min` mode, lr will</span>
<span class="sd">                be reduced when the quantity monitored has stopped</span>
<span class="sd">                decreasing; in `max` mode it will be reduced when the</span>
<span class="sd">                quantity monitored has stopped increasing. Default: &#39;min&#39;.</span>
<span class="sd">            factor : float</span>
<span class="sd">                Factor by which the learning rate will be</span>
<span class="sd">                reduced. new_lr = lr * factor. Default: 0.1.</span>
<span class="sd">            patience : int</span>
<span class="sd">                Number of epochs with no improvement after</span>
<span class="sd">                which learning rate will be reduced. For example, if</span>
<span class="sd">                `patience = 2`, then we will ignore the first 2 epochs</span>
<span class="sd">                with no improvement, and will only decrease the LR after the</span>
<span class="sd">                3rd epoch if the loss still hasn&#39;t improved then.</span>
<span class="sd">                Default: 10.</span>
<span class="sd">            verbose : bool</span>
<span class="sd">                If ``True``, prints a message to stdout for</span>
<span class="sd">                each update. Default: ``False``.</span>
<span class="sd">            threshold : float</span>
<span class="sd">                Threshold for measuring the new optimum,</span>
<span class="sd">                to only focus on significant changes. Default: 1e-4.</span>
<span class="sd">            threshold_mode : string</span>
<span class="sd">                One of `rel`, `abs`. In `rel` mode,</span>
<span class="sd">                dynamic_threshold = best * ( 1 + threshold ) in &#39;max&#39;</span>
<span class="sd">                mode or best * ( 1 - threshold ) in `min` mode.</span>
<span class="sd">                In `abs` mode, dynamic_threshold = best + threshold in</span>
<span class="sd">                `max` mode or best - threshold in `min` mode. Default: &#39;rel&#39;.</span>
<span class="sd">            cooldown : int</span>
<span class="sd">                Number of epochs to wait before resuming</span>
<span class="sd">                normal operation after lr has been reduced. Default: 0.</span>
<span class="sd">            min_lr : float or list</span>
<span class="sd">                A scalar or a list of scalars. A</span>
<span class="sd">                lower bound on the learning rate of all param groups</span>
<span class="sd">                or each group respectively. Default: 0.</span>
<span class="sd">            eps : float</span>
<span class="sd">                Minimal decay applied to lr. If the difference</span>
<span class="sd">                between new and old lr is smaller than eps, the update is</span>
<span class="sd">                ignored. Default: 1e-8</span>

<span class="sd">            &quot;&quot;&quot;</span>
            <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">ReduceLROnPlateau</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="n">factor</span><span class="p">,</span> <span class="n">patience</span><span class="p">,</span>
                                               <span class="n">verbose</span><span class="p">,</span> <span class="n">threshold</span><span class="p">,</span> <span class="n">threshold_mode</span><span class="p">,</span>
                                               <span class="n">cooldown</span><span class="p">,</span> <span class="n">min_lr</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span>

<div class="viewcode-block" id="ReduceLROnPlateauCallback.at_epoch_end"><a class="viewcode-back" href="../../../../_api/_build/delira/training/callbacks.html#delira.training.callbacks.ReduceLROnPlateauCallback.at_epoch_end">[docs]</a>        <span class="k">def</span> <span class="nf">at_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span>
                         <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
            <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">            Executes a single scheduling step</span>

<span class="sd">            Parameters</span>
<span class="sd">            ----------</span>
<span class="sd">            trainer : :class:`PyTorchNetworkTrainer`</span>
<span class="sd">                the trainer class, which can be changed</span>
<span class="sd">            kwargs :</span>
<span class="sd">                additional keyword arguments</span>

<span class="sd">            Returns</span>
<span class="sd">            -------</span>
<span class="sd">            :class:`PyTorchNetworkTrainer`</span>
<span class="sd">                modified trainer</span>

<span class="sd">            &quot;&quot;&quot;</span>
            <span class="n">metrics</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;val_metrics&quot;</span><span class="p">,</span> <span class="p">{})</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;val_score_key&quot;</span><span class="p">,</span>
                                                                   <span class="kc">None</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">metrics</span><span class="o">=</span><span class="n">metrics</span><span class="p">)</span>

            <span class="k">return</span> <span class="n">trainer</span></div></div>

<div class="viewcode-block" id="CosineAnnealingLRCallback"><a class="viewcode-back" href="../../../../_api/_build/delira/training/callbacks.html#delira.training.callbacks.CosineAnnealingLRCallback">[docs]</a>    <span class="k">class</span> <span class="nc">CosineAnnealingLRCallback</span><span class="p">(</span><span class="n">DefaultPyTorchSchedulerCallback</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Wraps PyTorch&#39;s `CosineAnnealingLR` Scheduler as callback</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">T_max</span><span class="p">,</span> <span class="n">eta_min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">last_epoch</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
            <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">            Parameters</span>
<span class="sd">            ----------</span>
<span class="sd">            optimizer : optimizer</span>
<span class="sd">                Wrapped optimizer.</span>
<span class="sd">            T_max : int</span>
<span class="sd">                Maximum number of iterations.</span>
<span class="sd">            eta_min : float</span>
<span class="sd">                Minimum learning rate. Default: 0.</span>
<span class="sd">            last_epoch : int</span>
<span class="sd">                The index of last epoch. Default: -1.</span>

<span class="sd">            &quot;&quot;&quot;</span>
            <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">CosineAnnealingLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">T_max</span><span class="p">,</span> <span class="n">eta_min</span><span class="p">,</span>
                                               <span class="n">last_epoch</span><span class="p">)</span></div>

<div class="viewcode-block" id="ExponentialLRCallback"><a class="viewcode-back" href="../../../../_api/_build/delira/training/callbacks.html#delira.training.callbacks.ExponentialLRCallback">[docs]</a>    <span class="k">class</span> <span class="nc">ExponentialLRCallback</span><span class="p">(</span><span class="n">DefaultPyTorchSchedulerCallback</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Wraps PyTorch&#39;s `ExponentialLR` Scheduler as callback</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">last_epoch</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
            <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">            Parameters</span>
<span class="sd">            ----------</span>
<span class="sd">            optimizer : Optimizer</span>
<span class="sd">                Wrapped optimizer.</span>
<span class="sd">            gamma : float</span>
<span class="sd">                Multiplicative factor of learning rate decay.</span>
<span class="sd">            last_epoch : int</span>
<span class="sd">                The index of last epoch. Default: -1.</span>

<span class="sd">            &quot;&quot;&quot;</span>
            <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">ExponentialLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">last_epoch</span><span class="p">)</span></div>

<div class="viewcode-block" id="LambdaLRCallback"><a class="viewcode-back" href="../../../../_api/_build/delira/training/callbacks.html#delira.training.callbacks.LambdaLRCallback">[docs]</a>    <span class="k">class</span> <span class="nc">LambdaLRCallback</span><span class="p">(</span><span class="n">DefaultPyTorchSchedulerCallback</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Wraps PyTorch&#39;s `LambdaLR` Scheduler as callback</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">lr_lambda</span><span class="p">,</span> <span class="n">last_epoch</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
            <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">            Parameters</span>
<span class="sd">            ----------</span>
<span class="sd">            optimizer : Optimizer</span>
<span class="sd">                Wrapped optimizer.</span>
<span class="sd">            lr_lambda : function or list</span>
<span class="sd">                A function which computes a multiplicative</span>
<span class="sd">                factor given an integer parameter epoch, or a list of such</span>
<span class="sd">                functions, one for each group in optimizer.param_groups.</span>
<span class="sd">            last_epoch : int</span>
<span class="sd">                The index of last epoch. Default: -1.</span>

<span class="sd">            &quot;&quot;&quot;</span>
            <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">LambdaLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">lr_lambda</span><span class="p">,</span> <span class="n">last_epoch</span><span class="p">)</span></div>

<div class="viewcode-block" id="MultiStepLRCallback"><a class="viewcode-back" href="../../../../_api/_build/delira/training/callbacks.html#delira.training.callbacks.MultiStepLRCallback">[docs]</a>    <span class="k">class</span> <span class="nc">MultiStepLRCallback</span><span class="p">(</span><span class="n">DefaultPyTorchSchedulerCallback</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Wraps PyTorch&#39;s `MultiStepLR` Scheduler as callback</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">milestones</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">last_epoch</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
            <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">            Parameters</span>
<span class="sd">            ----------</span>
<span class="sd">            optimizer : Optimizer</span>
<span class="sd">                Wrapped optimizer.</span>
<span class="sd">            milestones : list</span>
<span class="sd">                List of epoch indices. Must be increasing.</span>
<span class="sd">            gamma : float</span>
<span class="sd">                Multiplicative factor of learning rate decay.</span>
<span class="sd">                Default: 0.1.</span>
<span class="sd">            last_epoch : int</span>
<span class="sd">                The index of last epoch. Default: -1.</span>

<span class="sd">            &quot;&quot;&quot;</span>
            <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">MultiStepLR</span><span class="p">(</span>
                <span class="n">optimizer</span><span class="p">,</span> <span class="n">milestones</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">last_epoch</span><span class="p">)</span></div>

<div class="viewcode-block" id="StepLRCallback"><a class="viewcode-back" href="../../../../_api/_build/delira/training/callbacks.html#delira.training.callbacks.StepLRCallback">[docs]</a>    <span class="k">class</span> <span class="nc">StepLRCallback</span><span class="p">(</span><span class="n">DefaultPyTorchSchedulerCallback</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Wraps PyTorch&#39;s `StepLR` Scheduler as callback</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">step_size</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">last_epoch</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
            <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">            Parameters</span>
<span class="sd">            ----------</span>
<span class="sd">            optimizer : Optimizer</span>
<span class="sd">                Wrapped optimizer.</span>
<span class="sd">            step_size : int</span>
<span class="sd">                Period of learning rate decay.</span>
<span class="sd">            gamma :float</span>
<span class="sd">                Multiplicative factor of learning rate decay.</span>
<span class="sd">                Default: 0.1.</span>
<span class="sd">            last_epoch : int</span>
<span class="sd">                The index of last epoch. Default: -1</span>

<span class="sd">            &quot;&quot;&quot;</span>
            <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">StepLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">step_size</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">last_epoch</span><span class="p">)</span></div>

<span class="k">except</span> <span class="ne">ImportError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="k">raise</span> <span class="n">e</span>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Justus Schock, Oliver Rippel, Christoph Haarburger

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../../../_static/language_data.js"></script>
    

  

  <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>